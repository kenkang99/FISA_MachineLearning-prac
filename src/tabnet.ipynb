{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhy3Ru6OYW46wCSw6CP8wm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"z6k8LGwtmGgK"},"outputs":[],"source":["pip install pytorch-tabnet"]},{"cell_type":"code","source":["!git clone https://github.com/kenkang99/FISA_MachineLearning-prac.git"],"metadata":{"id":"mxKKYoW0mHPo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","data_orig = pd.read_csv('FISA_MachineLearning-prac/data/train.csv')\n","data_orig"],"metadata":{"id":"IcoyBGrBmKp9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = data_orig.drop('ID', axis=1)\n","data"],"metadata":{"id":"tUQhI_1ZmN_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Separate features (X) and target (y)\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Split the data into training and validation sets (80/20)\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(\"Training set shape:\", X_train.shape, y_train.shape)\n","print(\"Validation set shape:\", X_valid.shape, y_valid.shape)"],"metadata":{"id":"nPIyOvvlmPZK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import f1_score\n","from pytorch_tabnet.tab_model import TabNetClassifier\n","from pytorch_tabnet.metrics import Metric\n","\n","# 1) 멀티클래스용 Macro-F1 (권장)\n","class MacroF1(Metric):\n","    def __init__(self):\n","        self._name = \"macro_f1\"\n","        self._maximize = True  # 값이 클수록 좋은 지표인 경우 True\n","\n","    def __call__(self, y_true, y_score):\n","        \"\"\"\n","        y_true: (N,) int labels\n","        y_score: (N, C) class probabilities/logits for classification\n","        \"\"\"\n","        # TabNet은 eval 시 보통 (N, C) 점수를 줍니다. argmax로 라벨 변환\n","        y_pred = np.argmax(y_score, axis=1)\n","        return f1_score(y_true, y_pred, average=\"macro\")\n"],"metadata":{"id":"LmTJAm23mTSz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import f1_score\n","from pytorch_tabnet.tab_model import TabNetClassifier\n","from pytorch_tabnet.metrics import Metric\n","import torch\n","\n","# --- 커스텀 Macro-F1 Metric (TabNet 내부 모니터링용) ---\n","class MacroF1(Metric):\n","    def __init__(self):\n","        self._name = \"macro_f1\"\n","        self._maximize = True  # 값이 클수록 좋은 지표\n","\n","    def __call__(self, y_true, y_score):\n","        # y_score: (N, C) class probabilities/logits\n","        y_pred = np.argmax(y_score, axis=1)\n","        return f1_score(y_true, y_pred, average=\"macro\")\n","\n","\n","# 1) 스케일링 -> float32 보장\n","scaler = StandardScaler()\n","X_train1 = scaler.fit_transform(X_train).astype(np.float32)\n","X_valid1 = scaler.transform(X_valid).astype(np.float32)\n","\n","# 2) 라벨 -> int64 1D\n","to_int64 = lambda y: (y.to_numpy() if hasattr(y, \"to_numpy\") else np.asarray(y)).astype(np.int64).ravel()\n","y_train1 = to_int64(y_train)\n","y_valid1 = to_int64(y_valid)\n","\n","# --- CV 설정 ---\n","cv = 10\n","skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n","\n","best_f1 = -1.0\n","best_clf = None\n","\n","fold = 0\n","for tr_idx, va_idx in skf.split(X_train1, y_train1):\n","    fold += 1\n","    X_tr, y_tr = X_train1[tr_idx], y_train1[tr_idx]\n","    X_va, y_va = X_train1[va_idx], y_train1[va_idx]\n","\n","    clf_fold = TabNetClassifier()\n","\n","    # TabNet의 early stopping/모니터링은 커스텀 metric으로 진행\n","    clf_fold.fit(\n","        X_tr, y_tr,\n","        eval_set=[(X_va, y_va)],\n","        eval_name=[\"valid\"],\n","        eval_metric=[MacroF1],   # 커스텀 Metric 인스턴스\n","        max_epochs=100,\n","        patience=10,\n","        batch_size=128,\n","        virtual_batch_size=128,\n","        # num_workers=0  # (필요시 설정)\n","        # drop_last=False\n","    )\n","\n","    # fold 성능을 macro-F1로 직접 확인\n","    va_pred = clf_fold.predict(X_va)\n","    va_f1 = f1_score(y_va, va_pred, average=\"macro\")\n","    print(f\"[Fold {fold:02d}] macro-F1: {va_f1:.5f}\")\n","\n","    # 최고 성능 모델 갱신\n","    if va_f1 > best_f1:\n","        best_f1 = va_f1\n","        best_clf = clf_fold  # 가장 성능 좋은 모델 객체를 유지\n","\n","# --- 최종 clf를 최고 성능 fold의 모델로 지정 ---\n","clf = best_clf\n","print(f\"\\nBest CV macro-F1: {best_f1:.5f}\")\n","\n","# --- 홀드아웃(원래 valid) 성능 확인 및 예측 ---\n","preds = clf.predict(X_valid1)\n","holdout_f1 = f1_score(y_valid1, preds, average=\"macro\")\n","print(f\"Holdout macro-F1: {holdout_f1:.5f}\")\n"],"metadata":{"id":"NY1Wi5vWmV98"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","feature_names = list(X_train.columns)  # 시각화/정렬용\n","fi = clf.feature_importances_          # shape: (n_features,)\n","\n","# 상위 Top-K 출력\n","K = 30\n","order = np.argsort(fi)[::-1][:K]\n","for i in range(len(order)):\n","    print(f\"{i+1:>2}. {feature_names[order[i]]}: {fi[order[i]]:.10f}\")\n"],"metadata":{"id":"Lr3WR8NBmcqd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = pd.read_csv('/content/FISA_MachineLearning-prac/data/test.csv')\n","\n","test_x = test.drop(columns=['ID'])\n","preds = clf.predict(test_x.to_numpy(dtype=np.float32))\n","\n","submission = pd.read_csv('/content/FISA_MachineLearning-prac/data/sample_submission.csv')\n","\n","submission['target'] = preds\n","submission\n","\n","submission.to_csv('./baseline_submit_tabnet_cv.csv', index=False, encoding='utf-8-sig')"],"metadata":{"id":"3HpPYmvrmiPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ks-qOYHBmPys"},"execution_count":null,"outputs":[]}]}